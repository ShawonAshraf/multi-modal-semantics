{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dmmXDUxAF6lU"
   },
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KDrgDwvKGIEc"
   },
   "source": [
    "## Exercise\n",
    "Documentation and code of CLIP: https://github.com/openai/CLIP\n",
    "\n",
    "In this notebook, you will be working with CLIP (Contrastive Language-Image Pre-Training) model. We will be using the implementation and the pre-trained model based on ViT. This is a visual transformer, see the paper for details, or the (upcoming) course slides on ILIAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "except ModuleNotFoundError:\n",
    "    !conda install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda update -n base -c defaults conda\n",
    "#conda install torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKWITHCOLAB = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "53N4k0pj_9qL"
   },
   "source": [
    "# Setup: Preparation for Colab\n",
    "***If you work on your own local environment, you can skip this part and go directly to the Warm-Up.***\n",
    "\n",
    "Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will install the `clip` package and its dependencies, and check if PyTorch 1.7.1 or later is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WORKWITHCOLAB:\n",
    "    !pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WORKWITHCOLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "    except ModuleNotFoundError:\n",
    "        !pip install google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvbRA3p8Auys",
    "outputId": "e8c04b6e-f218-4384-d591-4113a4aa48f3"
   },
   "outputs": [],
   "source": [
    "# Mount your Google Drive to the Colab VM:\n",
    "if WORKWITHCOLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # TODO: Save the unzipped assignment folder in a filder on your google Drive. \n",
    "    # TODO: Enter the foldername in your Drive where you have saved it, e.g. 'assignment4/'\n",
    "    FOLDERNAME = None\n",
    "    FOLDERNAME = \"MultimodalSemantics_SS2023\"\n",
    "    assert FOLDERNAME is not None, \"enter the foldername.\"\n",
    "\n",
    "    # Now that we've mounted your Drive, this ensures that\n",
    "    # the Python interpreter of the Colab VM can load\n",
    "    # python files from within it.\n",
    "    sys.path.append('/content/drive/MyDrive/{}'.format(FOLDERNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WORKWITHCOLAB:\n",
    "    %load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ng-WmyVmDKCG",
    "outputId": "714ce531-32a7-4ab9-bddf-4a6243e0d078"
   },
   "outputs": [],
   "source": [
    "if WORKWITHCOLAB:\n",
    "    !cd /content/drive/MyDrive/$FOLDERNAME/\n",
    "    !ls ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YPHN7PJgKOzb"
   },
   "source": [
    "# Setting up working with CLIP\n",
    "\n",
    "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BpdJkdBssk9",
    "outputId": "677c1de4-6533-425e-c2a6-d1bf98dd12a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (6.1.1)\n",
      "Requirement already satisfied: regex in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from ftfy) (0.2.6)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-_kmlo3e0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-_kmlo3e0\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from clip==1.0) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from clip==1.0) (4.65.0)\n",
      "Requirement already satisfied: torch in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from clip==1.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from clip==1.0) (0.15.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.6)\n",
      "Requirement already satisfied: filelock in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from torch->clip==1.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from torch->clip==1.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from torch->clip==1.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from torch->clip==1.0) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: numpy in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.24.3)\n",
      "Requirement already satisfied: requests in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from torchvision->clip==1.0) (2.29.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from torchvision->clip==1.0) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/kafka/miniconda3/envs/mmsem/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "# Install CLIP if you don't have it yet\n",
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1hkDT38hSaP",
    "outputId": "7e9b1252-88d6-423a-a851-a61e314dbbf2"
   },
   "outputs": [],
   "source": [
    "from pkg_resources import packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch vision version:  0.15.2\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(\"Torch vision version: \", torchvision.__version__)\n",
    "#import torch\n",
    "#torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary:\n",
    "#!conda update --yes pytorch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a warn(f\"Failed to load image Python extension: {e}\") error?\n",
    "# Fix:\n",
    "#!pip3 uninstall --yes torchvision\n",
    "#!pip3 install torchvision==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need additional packages for loading (and visualising) the images\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "try:\n",
    "    import skimage\n",
    "except ModuleNotFoundError:\n",
    "    !conda install --yes scikit-image \n",
    "    import skimage\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eFxgLV5HAEEw"
   },
   "source": [
    "# Loading the model and the image preprocessor\n",
    "\n",
    "`clip.available_models()` will list the names of available CLIP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLFS29hnhlY4",
    "outputId": "35de2e67-b019-4237-f210-bd7ff650241a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "clip.available_models()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "u_XPYndVH2Bn"
   },
   "source": [
    "We load the pre-trained ViT-B/32 `model` and the image preprocessor (`process`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBRVTY9lbGm8",
    "outputId": "5ded9cd4-946a-417f-d27d-2a04d8e4537d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 427,616,513\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-L/14\")\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda().eval()\n",
    "else:\n",
    "    model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ubgd3PW0CbrK"
   },
   "source": [
    "## Exercise: Language Perception of Colour II\n",
    "In the exercises, you will use the coda dataset as in the previous assignment: https://github.com/nala-cub/coda\n",
    "\n",
    "This exercise now evaluates VL-models on the colour prediction task. \n",
    "So, again, follow the instructions below. \n",
    "\n",
    "### Instructions\n",
    "* Load the coda dataset. We will only use the *object names* and *colours* for this exercise.\n",
    "* Take the same 20 object names from different categories (e.g., vegetable/fruits, vehicles, etc.) that you used in Assignment 2.\n",
    "* Use the reference data of colour attributions that you created in Assigment 2.\n",
    "* The task now is to use CLIP for predicting the top colours that for each of your objects, as explained below. \n",
    " \n",
    "#### Approach\n",
    "We will compare different methods that diverge in the information that the modalities capture, and will perform zero-shot classification to predict typical colours of objects using CLIP. <br/>\n",
    "Regarding the prompts mentioned below, see also the file `templates.yaml` of the coda dataset.\n",
    "\n",
    "##### Data Preparation\n",
    "You need to set up your coda data and the data structures to extract the information needed for the methods. See the setup of the `swig_data` conducted above for zero-shot classification. \n",
    "\n",
    "1. **CLIP-col**: \n",
    "   *  Visual embeddings of images of colours   <br/>\n",
    "      Retrieve images from the web of all target colours under study, and store them in `coloured_objects`. Do not forget to give the source of the images in your report.\n",
    "    * For the textual representations, adapt the prompts you used in the previous assignment, but use the phrases \"this colour/these colours\" instead of masking the colour words. \n",
    "      * For example, you could use the template \n",
    "     `\"Most {object_areis_pl} have this colour.\"` <br/>\n",
    "     \n",
    "2. **CLIP-Tcol**: Baseline to **CLIP-col** without using images:\n",
    "   * \"Visual embeddings\" are actually textual embeddings of the colour *words*   <br/>\n",
    "      That is, use the textual encoder to extract the \"visual embeddings\" of each colour term.  \n",
    "    * For the textual representations, follow CLIP-col.<br/>\n",
    "\n",
    "3. **CLIP-obj**: \n",
    "   Here, you first have to define your target classes. These will be the target colours. \n",
    "   * Visual embeddings of images of the target objects <br/>\n",
    "      These images are grey-scale drawings. You can retrieve corresponding images from the MultPIC database, or query others (report the source of the photos). \n",
    "   * For the textual representations use simple prompts, such as <br/>\n",
    "       `\"This object usually is {}.\"`, <br/>\n",
    "       `\"This object usually is {} coloured.\"`\n",
    "\n",
    "### Tasks 1: Analysis of the VL-Models\n",
    "1. Briefly give an overview (in a table) of the methods in terms of with which modality they encode the *target object* and the *target colour(s)*, respectively. \n",
    "  | Method    | Image | Language |\n",
    "  | ------    | ----- | -------- |\n",
    "  | CLIP-col  |   ?   |    ?     |\n",
    "  | CLIP-Tcol |   ?   |    ?     |\n",
    "  | CLIP-obj  |   ?   |    ?     |\n",
    "2. Compute the accuracy of each model on the colour prediction task using your small reference data (previous coda assignment).\n",
    "3. Compare the effectiveness of the models, and discuss the possible reasons for the differences. \n",
    "4. For each model, are there objects for which it predicts wrong colours? Why could this be the case?\n",
    "5. Summarise in a few sentences your conclusions regarding the colour knowledge one can learn from visual-linguistic representations (i.e., from the visual modality and from text use). Also take into account the different models. \n",
    "\n",
    "### Tasks 2: Comparison of Language Model  against VL-Model\n",
    "*See the pdf of the assignment.*\n",
    "\n",
    "### Task 3: Pen & Paper Questions\n",
    "*See the pdf of the assignment.*\n",
    "\n",
    "***Submit the following files through ILIAS***:\n",
    "* `03ex_vlmodels_clip.ipynb`: your completed jupyter notebook\n",
    "* `03ex_vlmodels_clip.[txt|md]`: a plain text file that contains your answers to the task and questions.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up coloured_objects<a name=\"setup_coloured_objects\" />\n",
    "#### EXAMPLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banana_gratismalvorlagen.png\n",
      "banana_wikipedia.jpg\n",
      "peach_supercoloring.png\n",
      "peeling_91.jpg\n",
      "PICTURE_42.png\n",
      "PICTURE_69.png\n",
      "PICTURE_693.png\n",
      "PICTURE_697.png\n",
      "PICTURE_707.png\n"
     ]
    }
   ],
   "source": [
    "# TODO: set up your paths.\n",
    "FOLDERNAME = \"data\"\n",
    "image_dir = '{}/coloured_objects/'.format(FOLDERNAME)\n",
    "data_dir = '{}/coloured_objects/'.format(FOLDERNAME)\n",
    "\n",
    "for filename in [filename for filename in os.listdir(data_dir)]:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMc1AXzBlhzm"
   },
   "outputs": [],
   "source": [
    "# # images in coda to use and some descriptions for them written by yourselves\n",
    "# descriptions = [\n",
    "#     (\"PICTURE_69.png\", \"A grey-scale photo of a banana .\"),\n",
    "#     (\"PICTURE_42.png\", \"A photo of a pear .\"),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colour_classes = ['yellow', 'blue', 'brown', 'pink', 'gray', 'black', 'red', 'green', 'orange', 'purple', 'white']\n",
    "# colour_class2idx = {c:idx for (idx,c) in enumerate(colour_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coloured_objects_data = {\n",
    "#     # Q: How to adapt to multi-label prediction?\n",
    "#     \"PICTURE_69.png\": \"yellow\",\n",
    "#     \"PICTURE_42.png\": \"green\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For zero-shot classification\n",
    "# # We want to use more than one prompt, and get a \n",
    "# # mean prompt embedding for them: \n",
    "# colour_templates = [\"A photo of an object that is usually {} coloured .\", \n",
    "#                    \"The color of this object on the photo usually is {} .\"]\n",
    "# print(f\"{len(colour_classes)} classes, {len(colour_templates)} templates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset coda (/home/kafka/.cache/huggingface/datasets/corypaik___coda/default/1.0.1/cebd02670e856cb431040f8050cc5e0dcf887096aa9a62857be5596abfe23656)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69cc3e2d8aa4b9c9eb987bf12c374b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## YOUR CODE COMES HERE\n",
    "\n",
    "# load coda\n",
    "from datasets import load_dataset\n",
    "\n",
    "coda_dataset = load_dataset(\"corypaik/coda\")\n",
    "coda_dataset = coda_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_id': Value(dtype='string', id=None),\n",
       " 'display_name': Value(dtype='string', id=None),\n",
       " 'ngram': Value(dtype='string', id=None),\n",
       " 'label': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None),\n",
       " 'object_group': ClassLabel(names=['Single', 'Multi', 'Any'], id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'template_group': ClassLabel(names=['clip-imagenet', 'text-masked'], id=None),\n",
       " 'template_idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coda_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 20\n",
    "# selected_indexes = np.random.choice(len(object_names), 20)\n",
    "# selected_objects = [object_names[idx] for idx in selected_indexes]\n",
    "# print(selected_objects)\n",
    "\n",
    "selected_objects = ['Ambulance', 'Crow', 'Ketchup', 'Mobile phone', 'Porcupine', \n",
    "                    'Apple', 'Microphone', 'Skyscraper', 'Limousine',\n",
    "                    'Canoe', 'Light bulb', 'Asparagus', 'Violin', 'Sea turtle', \n",
    "                    'Skull', 'Giraffe', 'Traffic Light', 'Drawer', \n",
    "                    'Jeans', 'Flashlight']\n",
    "\n",
    "assert len(selected_objects) == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_colors = [\"white\", \"black\", \"red\", \"black\", \"brown\", \"red\", \"black\", \n",
    "                    \"gray\", \"white\", \"white\", \"yellow\", \"green\", \"black\", \n",
    "                    \"green\", \"white\", \"yellow\", \"red\", \"white\", \"blue\", \"white\"]\n",
    "\n",
    "assert len(selected_objects) == len(reference_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "04ex2_vlmodels_clip_new.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06e3f1fab5124aae8fb25ea107eafa37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1977a18716d94473aa729c45a0ecbe06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97aba6d50a5346b18bc955027c7d3010",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de7548d9903442e19fe57ddae3d28b0c",
      "value": 169001437
     }
    },
    "23aa4e3987df4e908e97dcb956a449ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_999455f40c754dc59c7875c07ba34608",
      "placeholder": "​",
      "style": "IPY_MODEL_b74aeb47ece24787a5973b20dcb6111e",
      "value": " 169001984/? [00:13&lt;00:00, 14267385.03it/s]"
     }
    },
    "2f6c877a4c4c4be6871626bb42927343": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "312e858be8c04d97b11266e21da7b3e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34a698dfb97749a9b48b912c63f5a42f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4316563c46af4ba3880f271e1b575c94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58deea131d8743e985058d1ea994432d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_824c6bdc49e0412a88452fa330df4d2e",
       "IPY_MODEL_79110b37df4a4396b891ed9fed8915a8",
       "IPY_MODEL_6a48cdb13f544e7e810cd5e00591cdd6"
      ],
      "layout": "IPY_MODEL_34a698dfb97749a9b48b912c63f5a42f"
     }
    },
    "6a48cdb13f544e7e810cd5e00591cdd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aacf5fddb5f64205a2a837fddf0ba325",
      "placeholder": "​",
      "style": "IPY_MODEL_e159e4b3daf942d3ba30ee3e08b899b3",
      "value": " 14/14 [00:00&lt;00:00, 48.97it/s]"
     }
    },
    "79110b37df4a4396b891ed9fed8915a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4316563c46af4ba3880f271e1b575c94",
      "max": 14,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_06e3f1fab5124aae8fb25ea107eafa37",
      "value": 14
     }
    },
    "824c6bdc49e0412a88452fa330df4d2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da6a44a02a6f47a09a81d994358d4a8b",
      "placeholder": "​",
      "style": "IPY_MODEL_d4b074eff23446f383e15e2743c56a06",
      "value": "100%"
     }
    },
    "97aba6d50a5346b18bc955027c7d3010": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "999455f40c754dc59c7875c07ba34608": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b337c30e9044de1aac741a6f49bf348": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aacf5fddb5f64205a2a837fddf0ba325": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b74aeb47ece24787a5973b20dcb6111e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb9c49265d9f4f219d4ec73656834361": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_312e858be8c04d97b11266e21da7b3e1",
      "placeholder": "​",
      "style": "IPY_MODEL_2f6c877a4c4c4be6871626bb42927343",
      "value": ""
     }
    },
    "cb846bae995f446f9932297f3d827958": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb9c49265d9f4f219d4ec73656834361",
       "IPY_MODEL_1977a18716d94473aa729c45a0ecbe06",
       "IPY_MODEL_23aa4e3987df4e908e97dcb956a449ed"
      ],
      "layout": "IPY_MODEL_9b337c30e9044de1aac741a6f49bf348"
     }
    },
    "d4b074eff23446f383e15e2743c56a06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da6a44a02a6f47a09a81d994358d4a8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de7548d9903442e19fe57ddae3d28b0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e159e4b3daf942d3ba30ee3e08b899b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
